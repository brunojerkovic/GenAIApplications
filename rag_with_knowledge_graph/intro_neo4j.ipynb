{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the text (will be used for later):",
   "id": "9537bfcf8b619803"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:15:40.479455Z",
     "start_time": "2024-07-06T15:15:40.456301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "getlanded.com | getlanded.etsy.com | getlanded@gmail.com\n",
    "(With page numbers where you can find more info in the included “Guide to Landing the Job”)\n",
    "#1. CREATE AN UNFORGETTABLE RESUME\n",
    "c Focus your resume around your target job /6\n",
    "c Include keywords from the job posting(s) to prove you’re a great match /10\n",
    "c Figure out what’s important to the employer /13\n",
    "c Demonstrate how you can provide value based on what the employer values /16\n",
    "c Create bullets based on your achievements, not just your duties /19\n",
    "c Quantify those achievement-based bullets with numbers /23\n",
    "c Start your bullets with a Success Verb /27\n",
    "c Make sure your formatting is consistent (fonts, bullets, periods, etc.) /32\n",
    "c Shorten your LinkedIn URL /32\n",
    "c Use past and present tense correctly /33\n",
    "c Avoid third person /33\n",
    "c Prioritize your information /33\n",
    "c PROOFREAD your resume!! /33\n",
    "c Make sure your email address is professional /33\n",
    "c Include your location: just city and state is best /33\n",
    "c Create a killer Professional Profile, since it gets read first /34\n",
    "c Put your job title first, not the company name /37\n",
    "Quick Tip CHECKLIST\n",
    "getlanded.com | getlanded.etsy.com | getlanded@gmail.com\n",
    "#2. ENSURE IT ACTUALLY GETS SEEN\n",
    "c Apply to jobs you are at least an 80% match for /41\n",
    "c Include keywords from the job posting so your resume actually gets chosen /10\n",
    "c Apply using less popular job sites to maximize your chances of standing out /41\n",
    "c If you apply using Indeed, use your own uploaded resume, not their version /42\n",
    "c Fill out every single field of the job application /42\n",
    "c When you upload to job postings, make sure the resume they spit back at you is\n",
    "correct, or copy/paste yourself from your Word/Pages file /42\n",
    "c Upload to job postings as a PDF file, NOT a Word/Pages file /42\n",
    "c Create a unique cover letter to entice the reader to read your resume /43\n",
    "c Spend HALF your time networking! Get referred to the job by someone you\n",
    "know, or connect with an actual person at the company /47\n",
    "Quick Tip CHECKLIST\n",
    "\"\"\""
   ],
   "id": "eb9cc24b7308d641",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:16:56.821408Z",
     "start_time": "2024-07-06T15:16:56.790142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents import create_react_agent, AgentExecutor, Tool\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "load_dotenv()"
   ],
   "id": "c355f45be7eb0952",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Neo4j Intro: Manual Graph Creation\n",
    "\n",
    "Here, we will use langchain's neo4j interface to add nodes, relationships, and clear the database."
   ],
   "id": "9644d917b99eb7a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:16:57.709215Z",
     "start_time": "2024-07-06T15:16:57.693828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Class that can write nodes, relationships, and clear the graph\n",
    "class Neo4jDatabase:\n",
    "    def __init__(self):\n",
    "        self.graph = Neo4jGraph()\n",
    "        \n",
    "    def clear_database(self):\n",
    "        self.graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "    \n",
    "    def add_nodes_to_graph(self, nodes):\n",
    "        for node in nodes:\n",
    "            self.graph.query(\n",
    "                \"CREATE (n:Person {id: $id, name: $name})\",\n",
    "                {\"id\": node['id'], \"name\": node['name']}\n",
    "            )\n",
    "        \n",
    "    def add_relationship_to_graph(self, relationships):\n",
    "        for relationship in relationships:\n",
    "            self.graph.query(\n",
    "                \"\"\"\n",
    "                MATCH (a:Person {id: $start_id}), (b:Person {id: $end_id})\n",
    "                CREATE (a)-[:RELATIONSHIP_TYPE {type: $type, since: $since, strength: $strength}]->(b)\n",
    "                \"\"\",\n",
    "                {\n",
    "                    \"start_id\": relationship['start_id'],\n",
    "                    \"end_id\": relationship['end_id'],\n",
    "                    \"type\": relationship['type'],\n",
    "                    \"since\": relationship['since'],\n",
    "                    \"strength\": relationship['strength']\n",
    "                }\n",
    "            )"
   ],
   "id": "a278c500666f6df0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:00.394168Z",
     "start_time": "2024-07-06T15:16:58.797871Z"
    }
   },
   "source": [
    "# Init the database\n",
    "neo4j_db = Neo4jDatabase()\n",
    "\n",
    "# Clear the database\n",
    "neo4j_db.clear_database()\n",
    "\n",
    "# Add nodes to the graph\n",
    "nodes = [\n",
    "    {\"id\": 1, \"name\": \"Alice\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\"}\n",
    "]\n",
    "neo4j_db.add_nodes_to_graph(nodes)\n",
    "print(\"Nodes added successfully!\")\n",
    "\n",
    "# Add relationships to the graph\n",
    "relationships = [\n",
    "    {\"start_id\": 1, \"end_id\": 2, \"type\": \"KNOWS\", \"since\": \"2020\", \"strength\": \"high\"},\n",
    "    {\"start_id\": 2, \"end_id\": 3, \"type\": \"FRIENDS_WITH\", \"since\": \"2019\", \"strength\": \"medium\"}\n",
    "]\n",
    "neo4j_db.add_relationship_to_graph(relationships)\n",
    "print(\"Relationships added successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes added successfully!\n",
      "Relationships added successfully!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. LLM-Based Graph Creation from Documents\n",
    "\n",
    "Here we will automatically create graph database (in neo4j) from a document. Then, we will filter the nodes and relationships in langchain."
   ],
   "id": "f78471ad1e100614"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:21.785470Z",
     "start_time": "2024-07-06T15:17:13.687534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Init LLM and a transformer to create Cypher query\n",
    "llm = ChatOpenAI()\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# Read the documents and convert to graph docs\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "# Print created nodes\n",
    "print(\"Unfiltered nodes:\")\n",
    "for node in graph_documents[0].nodes:\n",
    "    print(node)\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Filter the nodes with another LLMGraphTransformer\n",
    "llm_transformer_filtered = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=[\"Resume tip\", \"Job advice\"],\n",
    "    allowed_relationships=[\"JOB_APPLICATION_TIP\", \"RESUME_TIP\"]\n",
    ")\n",
    "graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(documents)\n",
    "\n",
    "# Print filtered nodes\n",
    "print(\"Filtered nodes:\")\n",
    "for node in graph_documents_filtered[0].nodes:\n",
    "    print(node)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Upload graph to Neo4j\n",
    "graph = Neo4jGraph()\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "graph.add_graph_documents(graph_documents)\n",
    "print(\"Successfully uploaded!\")\n",
    "# Now, we created a graph, but it has no semantic understanding yet"
   ],
   "id": "7273c113a0cf2dec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered nodes:\n",
      "id='Resume' type='Document'\n",
      "id='Email' type='Communication'\n",
      "id='Linkedin' type='Platform'\n",
      "id='Professional Profile' type='Information'\n",
      "id='Location' type='Information'\n",
      "id='Cover Letter' type='Document'\n",
      "\n",
      "\n",
      "Filtered nodes:\n",
      "id='Create_An_Unforgettable_Resume' type='Resume tip'\n",
      "id='Ensure_It_Actually_Gets_Seen' type='Resume tip'\n",
      "\n",
      "\n",
      "Successfully uploaded!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:26.225630Z",
     "start_time": "2024-07-06T15:17:24.062559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create vector database (index) for certain node properties (creates embeddings and uploads it to the online database)\n",
    "# This will help user-query when using an LLM over the graph-database\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-ada-002\"),\n",
    "    node_label=\"Information\",\n",
    "    text_node_properties=['id'],\n",
    "    embedding_node_property='info', # Node attribute where it will be saved\n",
    "    index_name=\"vector3\"  # Watch out that the same index needs to have the same dimension (myb use the same model for the same index)\n",
    ")  # This returns vector index (which has no awareness of graph structure; it's simply a vector embedding of a dict of 'text_node_properties')"
   ],
   "id": "689bdfe98ed293c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 73, offset: 72} for query: \"UNWIND $data AS row MATCH (n:`Information`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'info', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1. Similarity Search\n",
    "\n",
    "Here we will perform a similarity search between a **query** and an **embedding**. This is not yet RAG, as we are not actually using an LLM for generation (the output of this would usually be a context that would go to the LLM in a standard RAG application)."
   ],
   "id": "7e3d7ca81fb1a027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:28.260713Z",
     "start_time": "2024-07-06T15:17:27.501877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform similarity search (on the vector index)\n",
    "response = vector_index.similarity_search(\n",
    "    \"What info is good?\"\n",
    ")\n",
    "print(\"Information: \", [r.page_content.split(':')[1].strip() for r in response])\n",
    "\n",
    "# Perform Cypher query (on graph database)\n",
    "response = graph.query(\n",
    "    \"MATCH (t:Platform) RETURN count(*)\"\n",
    ")\n",
    "print(\"Platforms: \", response)"
   ],
   "id": "46fed098a0370bff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information:  ['Professional Profile', 'Location']\n",
      "Platforms:  [{'count(*)': 1}]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. RAG (for vector-index chain)\n",
    "\n",
    "\n",
    "Here we will perform RAG. For that, we need to init a new LLM to do it. Here the output (that we saw previously) is used as a context for a new pass to the LLM. **Keep in mind that this only has access to the index, not the graph database structure!!!**"
   ],
   "id": "cbdbb1f0803fa8b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:42.068692Z",
     "start_time": "2024-07-06T15:17:39.910260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# QA-chain (for QA retrieval)\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_index.as_retriever()\n",
    ")\n",
    "\n",
    "# Perform RAG (on the vector QA chain for retrieval)\n",
    "response = vector_qa.invoke(\n",
    "    \"What platforms can I use for job search based on the knowledge graph?\"\n",
    ")\n",
    "print(\"Answer: \", response)"
   ],
   "id": "c1520841f4049637",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  {'query': 'What platforms can I use for job search based on the knowledge graph?', 'result': 'Based on the knowledge graph, you can use platforms like LinkedIn, Indeed, Glassdoor, Monster, and CareerBuilder for job searches.'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another way to create a vector-QA chain (this is more novel):",
   "id": "67125a8c9d4fd8ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "\n",
    "# Get the prompt for retrieval creation\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "# Create the chain\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    ChatOpenAI(), retrieval_qa_chat_prompt\n",
    ")\n",
    "vector_qa = create_retrieval_chain(retriever=vector_index.as_retriever(), combine_docs_chain=combine_docs_chain)\n",
    "\n",
    "# Perform RAG (on the vector QA chain for retrieval)\n",
    "response = vector_qa.invoke({\"input\":\n",
    "    \"What platforms can I use for job search based on the knowledge graph?\"\n",
    "})\n",
    "print(\"Answer: \", response)"
   ],
   "id": "ca1bfe748ab2768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.3. LLM-Cypher Chain\n",
    "\n",
    "Here we will create another chain. This one will only have access to the graph database structure."
   ],
   "id": "fbce5036225e25ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:49.371704Z",
     "start_time": "2024-07-06T15:17:47.769154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Updates the schema to get the latest schema version inside the local cache\n",
    "graph.refresh_schema()\n",
    "\n",
    "# Init the chain (this will first perform a Cypher query, and then after that it will perform the QA query)\n",
    "# Take into account that this is a chain, not an agent\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = ChatOpenAI(temperature=0, model_name='gpt-4'), # Translates the user-query to Cypher query (use more advanced model for this)\n",
    "    qa_llm = ChatOpenAI(temperature=0), graph=graph, verbose=True,  # Answers the question using an LLM whose context is the output of the previous Cypher query\n",
    ")"
   ],
   "id": "d902ef0d88c6d25a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:17:56.590984Z",
     "start_time": "2024-07-06T15:17:55.312716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the chain\n",
    "answer = cypher_chain.invoke(\n",
    "    \"What platforms exist?\"\n",
    ")\n",
    "print(\"ANSWER 1\", answer)"
   ],
   "id": "28540b204f71c09a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new GraphCypherQAChain chain...\u001B[0m\n",
      "Generated Cypher:\n",
      "\u001B[32;1m\u001B[1;3mMATCH (p:Platform) RETURN p.id\u001B[0m\n",
      "Full Context:\n",
      "\u001B[32;1m\u001B[1;3m[{'p.id': 'Linkedin'}]\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "ANSWER 1 {'query': 'What platforms exist?', 'result': 'Linkedin'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:18:01.713165Z",
     "start_time": "2024-07-06T15:18:00.018633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the chain\n",
    "answer = cypher_chain.invoke(\n",
    "    \"How many information tips are there?\"\n",
    ")\n",
    "print(\"ANSWER 2: \", answer)"
   ],
   "id": "7866b1e7c278035f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new GraphCypherQAChain chain...\u001B[0m\n",
      "Generated Cypher:\n",
      "\u001B[32;1m\u001B[1;3mMATCH (d:Document)-[:CONTENT]->(i:Information) RETURN COUNT(i)\u001B[0m\n",
      "Full Context:\n",
      "\u001B[32;1m\u001B[1;3m[{'COUNT(i)': 2}]\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "ANSWER 2:  {'query': 'How many information tips are there?', 'result': 'There are 2 information tips.'}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4. LLM Agent with Cypher Tool (Final Product of KG-based RAG)\n",
    "\n",
    "Now we will combine the last two chains here. We will create an **agent** that has index both to the graph database chain (for structure) and to the vector-index (for the embeddings)."
   ],
   "id": "8cf83f1b2d34b4c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:18:45.795252Z",
     "start_time": "2024-07-06T15:18:44.640801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Init Cypher tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Information\",\n",
    "        func=vector_qa.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
    "        Not useful for counting the number of tasks.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "        handle_tool_error=True  # Good to have it on, in case there is an error, it will be handled by the agent (the tool will signal the agent instead of raising an exception)\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
    "        their dependencies or assigned people. Also useful for any sort of \n",
    "        aggregation like counting the number of tasks, etc.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "        handle_tool_error=True\n",
    "    ),\n",
    "]\n",
    "# tools = [vector_qa.run, cypher_chain.run]\n",
    "\n",
    "# Prompt\n",
    "prompt_agent = hub.pull(\"hwchase17/react\")\n",
    "llm = ChatOpenAI(temperature=0.2, model_name='gpt-4')\n",
    "\n",
    "# Init the agent\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    prompt_agent\n",
    ")\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools\n",
    ")"
   ],
   "id": "1e58bd785a1c41ae",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:18:50.224924Z",
     "start_time": "2024-07-06T15:18:45.795252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# User query 1\n",
    "query = \"What platforms exist?\"\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(response)"
   ],
   "id": "269520ede7dc05b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new GraphCypherQAChain chain...\u001B[0m\n",
      "Generated Cypher:\n",
      "\u001B[32;1m\u001B[1;3mMATCH (p:Platform) RETURN p.id\u001B[0m\n",
      "Full Context:\n",
      "\u001B[32;1m\u001B[1;3m[{'p.id': 'Linkedin'}]\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'input': 'What platforms exist?', 'output': 'The platform that exists is Linkedin.'}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:18:55.113091Z",
     "start_time": "2024-07-06T15:18:50.224924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# User query 2\n",
    "query = \"How many information tips are there?\"\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(response)"
   ],
   "id": "43b54b863666125d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new GraphCypherQAChain chain...\u001B[0m\n",
      "Generated Cypher:\n",
      "\u001B[32;1m\u001B[1;3mMATCH (d:Document)-[:CONTENT]->(i:Information) RETURN COUNT(i)\u001B[0m\n",
      "Full Context:\n",
      "\u001B[32;1m\u001B[1;3m[{'COUNT(i)': 2}]\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'input': 'How many information tips are there?', 'output': 'There are 2 information tips available.'}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c30259f1c56fc57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
