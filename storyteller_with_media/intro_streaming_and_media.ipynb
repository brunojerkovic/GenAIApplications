{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:07:19.242092Z",
     "start_time": "2024-07-08T06:07:19.211367Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d8561ab9042d7",
   "metadata": {},
   "source": [
    "# 1. Streaming Chat Responses - OpenAI API\n",
    "\n",
    "In this part, we will only use streaming responses. For OpenAI models, streaming is not yet supported in langchain (Jul 2024). Hence, we will use streaming from their original algorithm. However, langchain supports streaming for other models, so in future, it might also for OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5522517b4f54d49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:08:22.395016Z",
     "start_time": "2024-07-08T07:08:21.503320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Init client (reads API from environment variable)\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54d3ab201b8fa114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:09:26.715719Z",
     "start_time": "2024-07-08T06:09:26.153470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Streaming response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 100, with a comma between each number. E.g., 1, 2, 3,'}\n",
    "    ],\n",
    "    temperature=0.,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a17856c1aaef7e14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:12:47.355335Z",
     "start_time": "2024-07-08T06:12:47.325144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stream chat response\n",
    "history = \"\"\n",
    "for chunk in response:\n",
    "    clear_output(wait=False)\n",
    "    model_output = chunk.choices[0].delta.content\n",
    "    if model_output:\n",
    "        history += model_output\n",
    "    print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73175bef3dae3",
   "metadata": {},
   "source": [
    "# Image generation - OpenAI API\n",
    "\n",
    "DALL-E-3 is not yet supported by langchain. Hence, we will take a hybrid approach here. We will first generate a prompt with langchain's GPT interface. Then, we will use native OpenAI API for DALL-E-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c14b82fb21753497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:34:14.668193Z",
     "start_time": "2024-07-08T06:34:10.939443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a response of an image description\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=\"\"\"\n",
    "        Generate a detailed prompt to generate an image based on the following description in under 500 words.\n",
    "        The image should be in a style for a children's book.\n",
    "        The image description: '''{image_desc}'''\n",
    "    \"\"\",\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "image_description = chain.invoke(input={\"image_desc\": \"a white siamese cat\"})\n",
    "image_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "87ea6dbc947dc224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:17:16.698388Z",
     "start_time": "2024-07-08T07:17:16.670475Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt.format(image_desc=\"a white cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "adf1ac98c2dfa118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:34:14.683861Z",
     "start_time": "2024-07-08T06:34:14.668193Z"
    }
   },
   "outputs": [],
   "source": [
    "len(image_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d1c02adc2062b1be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:34:27.584019Z",
     "start_time": "2024-07-08T06:34:14.683861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a response of a Whisper\n",
    "response = client.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=image_description,\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",  # [\"standard\", \"hd\"]\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d70bc8b51abf273b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:34:27.599905Z",
     "start_time": "2024-07-08T06:34:27.587901Z"
    }
   },
   "outputs": [],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ad2ebf6d49f6",
   "metadata": {},
   "source": [
    "# 3. Text-to-speech - OpenAI API\n",
    "\n",
    "Keep in mind that OpenAI uses TTS model for this. On the other hand, Whisper is used for speech-to-text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68eda13a2d93e01d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:07:56.818790Z",
     "start_time": "2024-07-08T07:07:43.883096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get voice and save to a file\n",
    "with client.audio.speech.with_streaming_response.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"nova\",\n",
    "    input=image_description,\n",
    ") as response:\n",
    "    # Save voice\n",
    "    response.stream_to_file(\"output.mp3\")\n",
    "    \n",
    "# Later load as \"st.audio(output.mp3)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7651cc7",
   "metadata": {},
   "source": [
    "4. Speech-to-Text - OpenAI\n",
    "For this we need to use whisper. We can use the library \"openai-whisper\". For detailed info, look in project 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c36b71ec5e20036b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:59:31.913509Z",
     "start_time": "2024-07-08T06:59:31.897871Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: code for whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1ce891617f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
