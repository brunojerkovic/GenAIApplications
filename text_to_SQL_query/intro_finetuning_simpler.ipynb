{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T16:49:20.593087Z",
     "start_time": "2024-07-14T16:49:20.546205Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import evaluate\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finetuning for Classification",
   "id": "c8df97ef14e35e4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the data ready\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ],
   "id": "12e05512ce7ac49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:40:45.695342Z",
     "start_time": "2024-07-14T14:40:45.146652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the model (and trainer) ready\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\",\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(2_000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(100)),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "id": "2c9e73a55cd2293d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the outputs before training\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"].select(range(50)))\n",
    "print(\"Predictions shapes: \", predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Convert logits to predicted class\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "print(f\"Accuracy (before training): {accuracy['accuracy']}\")"
   ],
   "id": "59b4599bd145c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:55:28.096580Z",
     "start_time": "2024-07-14T14:40:46.803485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "e1e51838f9da05f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 14:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.6276907348632812, metrics={'train_runtime': 881.0138, 'train_samples_per_second': 2.27, 'train_steps_per_second': 0.284, 'total_flos': 76474872684480.0, 'train_loss': 0.6276907348632812, 'epoch': 1.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:40:39.350431Z",
     "start_time": "2024-07-14T14:40:34.248582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the outputs\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"].select(range(50)))\n",
    "print(\"Predictions shapes: \", predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Convert logits to predicted class\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "print(f\"Accuracy (after training): {accuracy['accuracy']}\")"
   ],
   "id": "9673b6fab27c0410",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shapes:  (50, 2) (50,)\n",
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T14:21:31.375840Z",
     "start_time": "2024-07-14T14:21:31.213234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference mode\n",
    "def inference(model, sentences: List[Tuple]):\n",
    "    sentences = [sentences] if not isinstance(sentences, list) else sentences\n",
    "    output_list = []\n",
    "    \n",
    "    for s1, s2 in sentences:\n",
    "        inputs = tokenizer(s1, s2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "        \n",
    "        # Save the outputs\n",
    "        output_list.append((predicted_class, probs.tolist()[0]))\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "# Example input strings\n",
    "sentences = [\n",
    "    (\"I am called Tom. Tom is hungry. Tom wants to eat fish.\", \"Mick is thirsty.\"),\n",
    "    (\"I am called Tom. Tom is hungry. Tom wants to eat fish.\", \"Tom is hungry.\")\n",
    "]\n",
    "\n",
    "inference(model, sentences)"
   ],
   "id": "5886cbd3112e279c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0.6112167835235596, 0.3887832760810852]),\n",
       " (1, [0.4407484531402588, 0.5592515468597412])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Finetuning for Regression (not tested; no space on disk)",
   "id": "5e2b615ec252bff9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T16:50:25.375092Z",
     "start_time": "2024-07-14T16:49:30.627741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the data ready\n",
    "raw_datasets = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "train_test_split = raw_datasets[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Create a new DatasetDict including the new split\n",
    "datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test'],\n",
    "    'test': raw_datasets['test']\n",
    "})\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"sql_prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    targets = tokenizer(examples[\"sql\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"label\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Perform tokenization\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ],
   "id": "10be01023404cbe3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5a154883d7f469cb3dbe91b4817cfd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Perform tokenization\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m tokenized_datasets \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenize_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForSeq2Seq(tokenizer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\dataset_dict.py:869\u001B[0m, in \u001B[0;36mDatasetDict.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001B[0m\n\u001B[0;32m    866\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[1;32m--> 869\u001B[0m     {\n\u001B[0;32m    870\u001B[0m         k: dataset\u001B[38;5;241m.\u001B[39mmap(\n\u001B[0;32m    871\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m    872\u001B[0m             with_indices\u001B[38;5;241m=\u001B[39mwith_indices,\n\u001B[0;32m    873\u001B[0m             with_rank\u001B[38;5;241m=\u001B[39mwith_rank,\n\u001B[0;32m    874\u001B[0m             input_columns\u001B[38;5;241m=\u001B[39minput_columns,\n\u001B[0;32m    875\u001B[0m             batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[0;32m    876\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    877\u001B[0m             drop_last_batch\u001B[38;5;241m=\u001B[39mdrop_last_batch,\n\u001B[0;32m    878\u001B[0m             remove_columns\u001B[38;5;241m=\u001B[39mremove_columns,\n\u001B[0;32m    879\u001B[0m             keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[0;32m    880\u001B[0m             load_from_cache_file\u001B[38;5;241m=\u001B[39mload_from_cache_file,\n\u001B[0;32m    881\u001B[0m             cache_file_name\u001B[38;5;241m=\u001B[39mcache_file_names[k],\n\u001B[0;32m    882\u001B[0m             writer_batch_size\u001B[38;5;241m=\u001B[39mwriter_batch_size,\n\u001B[0;32m    883\u001B[0m             features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m    884\u001B[0m             disable_nullable\u001B[38;5;241m=\u001B[39mdisable_nullable,\n\u001B[0;32m    885\u001B[0m             fn_kwargs\u001B[38;5;241m=\u001B[39mfn_kwargs,\n\u001B[0;32m    886\u001B[0m             num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[0;32m    887\u001B[0m             desc\u001B[38;5;241m=\u001B[39mdesc,\n\u001B[0;32m    888\u001B[0m         )\n\u001B[0;32m    889\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    890\u001B[0m     }\n\u001B[0;32m    891\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\dataset_dict.py:870\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    866\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[0;32m    869\u001B[0m     {\n\u001B[1;32m--> 870\u001B[0m         k: \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    871\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    872\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    873\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[43m            \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_file_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    882\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    883\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    884\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    886\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    887\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    888\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    889\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    890\u001B[0m     }\n\u001B[0;32m    891\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\arrow_dataset.py:602\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    600\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    601\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 602\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    603\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    604\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    605\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\arrow_dataset.py:567\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    560\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    565\u001B[0m }\n\u001B[0;32m    566\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 567\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    568\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    569\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\arrow_dataset.py:3161\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3156\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[0;32m   3157\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3158\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3159\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3160\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3161\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3162\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3163\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\arrow_dataset.py:3575\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3573\u001B[0m         writer\u001B[38;5;241m.\u001B[39mwrite_table(batch\u001B[38;5;241m.\u001B[39mto_arrow())\n\u001B[0;32m   3574\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3575\u001B[0m         \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3576\u001B[0m num_examples_progress_update \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m num_examples_in_batch\n\u001B[0;32m   3577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m _time \u001B[38;5;241m+\u001B[39m config\u001B[38;5;241m.\u001B[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\arrow_writer.py:572\u001B[0m, in \u001B[0;36mArrowWriter.write_batch\u001B[1;34m(self, batch_examples, writer_batch_size)\u001B[0m\n\u001B[0;32m    570\u001B[0m schema \u001B[38;5;241m=\u001B[39m inferred_features\u001B[38;5;241m.\u001B[39marrow_schema \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpa_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\n\u001B[0;32m    571\u001B[0m pa_table \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39mTable\u001B[38;5;241m.\u001B[39mfrom_arrays(arrays, schema\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m--> 572\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\datasets\\arrow_writer.py:590\u001B[0m, in \u001B[0;36mArrowWriter.write_table\u001B[1;34m(self, pa_table, writer_batch_size)\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_bytes \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mnbytes\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_examples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mnum_rows\n\u001B[1;32m--> 590\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpa_writer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\pyarrow\\ipc.pxi:529\u001B[0m, in \u001B[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\pyarrow\\error.pxi:88\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\fsspec\\implementations\\local.py:422\u001B[0m, in \u001B[0;36mLocalFileOpener.write\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrite\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 422\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T16:41:22.112576Z",
     "start_time": "2024-07-14T16:36:24.272863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\",\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(10)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(10)),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "id": "2b4da72397fe51a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\genai_simple_projects\\lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: Not enough free disk space to download the file. The expected file size is: 1198.63 MB. The target location C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--byt5-small\\blobs only has 807.65 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "511a1daa293f43ff8cfa936999ca8d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f8caad627c94f1493fd9e4bc084c39a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T16:15:22.484753Z",
     "start_time": "2024-07-14T16:14:48.445193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the outputs before training\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"].select(range(5)))\n",
    "# print(\"Predictions shapes: \", predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# Convert logits to predicted class\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Compute accuracy\n",
    "bleu = accuracy_metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "print(f\"Bleu (before training): {bleu['bleu']}\")"
   ],
   "id": "42ac3df211602221",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24196\\2181040938.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  preds = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 50265 at dim 3 (got 768)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m accuracy_metric \u001B[38;5;241m=\u001B[39m evaluate\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Convert logits to predicted class\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictions\u001B[49m\u001B[43m)\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Compute accuracy\u001B[39;00m\n\u001B[0;32m     12\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m accuracy_metric\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39mpreds, references\u001B[38;5;241m=\u001B[39mpredictions\u001B[38;5;241m.\u001B[39mlabel_ids)\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 50265 at dim 3 (got 768)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T15:38:48.497523Z",
     "start_time": "2024-07-14T15:37:48.493439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "4caaa9c902c6e340",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=15.858112335205078, metrics={'train_runtime': 59.6909, 'train_samples_per_second': 0.168, 'train_steps_per_second': 0.034, 'total_flos': 3048682291200.0, 'train_loss': 15.858112335205078, 'epoch': 1.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the outputs after training\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"].select(range(5)))\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# Convert logits to predicted class\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Compute accuracy\n",
    "bleu = accuracy_metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "print(f\"Bleu (after training): {bleu['bleu']}\")"
   ],
   "id": "bae52469d2bab897"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T15:47:26.152279Z",
     "start_time": "2024-07-14T15:47:26.129128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is to decode for inference\n",
    "\n",
    "# Decode the predictions and labels\n",
    "decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions.predictions]\n",
    "decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in predictions.label_ids]\n",
    "\n",
    "# Print the results\n",
    "for i in range(len(decoded_predictions)):\n",
    "    print(f\"Prediction: {decoded_predictions[i]}\")\n",
    "    print(f\"Label: {decoded_labels[i]}\")"
   ],
   "id": "9bf4af491a85483a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 49179, 44619, ...,     1,     1,     1],\n",
       "       [    0, 10089,  3850, ...,     1,     1,     1],\n",
       "       [    0, 49179,  1709, ...,     1,     1,     1],\n",
       "       ...,\n",
       "       [    0, 49179,  3893, ...,     1,     1,     1],\n",
       "       [    0, 49179,  1484, ...,     1,     1,     1],\n",
       "       [    0, 49179, 32464, ...,     1,     1,     1]], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
