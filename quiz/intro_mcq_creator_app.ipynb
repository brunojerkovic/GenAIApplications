{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install Libraries",
   "id": "3313438dd3c80c86"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-22T11:21:59.177406Z",
     "start_time": "2024-06-22T11:21:59.158354Z"
    }
   },
   "source": [
    "#!pip install unstructured\n",
    "#!pip install tiktoken\n",
    "#!pip install pinecone-client\n",
    "#!pip install pypdf"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:42:35.523553Z",
     "start_time": "2024-06-22T11:42:35.498550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "20304c7ba23f8963",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Document Loader",
   "id": "c9c11efeb31457c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:26:38.125922Z",
     "start_time": "2024-06-22T11:26:37.716739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = \"Docs/\"\n",
    "loader = PyPDFDirectoryLoader(directory)\n",
    "documents = loader.load()\n",
    "print(\"Number of original documents: \", len(documents))  # It says 3 because one page is one document (metadata contains page_number)\n",
    "print(\"Document example: \", documents[-1])"
   ],
   "id": "cb09c9a1c3e0e81c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original documents:  3\n",
      "Document example:  page_content=\"India's diplomatic influence is also growing on the global stage. The country actively \\nparticipates in international forums and has strong bilateral relations with nations around the \\nworld. India is a founding member of the Non-Aligned Movement and plays an active role in \\nvarious international organizations, such as the United Nations and World Trade Organization.\\nIn conclusion, India is a vast and diverse country with a rich cultural heritage, stunning \\nlandscapes, and a rapidly growing economy. It is a nation where ancient traditions coexist with \\nmodern aspirations. Despite its challenges, India continues to evolve and leave an indelible \\nmark on the world, making it a fascinating and dynamic country to explore.\" metadata={'source': 'Docs\\\\Doc 2.pdf', 'page': 1}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Document Transformer\n",
    "Here, we will use *RecursiveCharacterTextSplitter* instead of a simple *CharacterTextSplitter*, becasue the recursive one will first start to split the text naturally (at breakpoints on paragraphs, sentences, or other specified delimeters). After that, it will split the text on smaller chunks based on characters. **It ensures that each chunk is as meaningful as possible by preserving the text structure.**"
   ],
   "id": "4a33eaf0340ecb52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:26:39.572258Z",
     "start_time": "2024-06-22T11:26:39.560256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_size = 1_000\n",
    "chunk_overlap = 20\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(\"Documents length: \", len(docs))"
   ],
   "id": "e6ddd96e6b537385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents length:  7\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Data Embedding",
   "id": "6a16bcd4dc4b1ae6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:26:42.313896Z",
     "start_time": "2024-06-22T11:26:40.734885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Test the embeddings model\n",
    "embedding = embedding_model.embed_query(\"My test query to get embedded.\")\n",
    "print(\"Length of the embedding vector: \",len(embedding))"
   ],
   "id": "f0459f211559ef0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the embedding vector:  384\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Vector Database\n",
    "For this we will use PINECONE DB. First, go to their website and create an index there. Here, we will just connect to that index.\n",
    "Keep in mind, instead of using **original Pinecone**, we will use **langchain's proxy for Pinecone**."
   ],
   "id": "df25766ac320b593"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:38:20.000306Z",
     "start_time": "2024-06-22T11:38:17.433995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We will use only first two docs initially, and then the last two, but this is just to show how can Pinecone perform lazy adding\n",
    "index_name = \"mcq-creator\"\n",
    "index_pc = PineconeVectorStore.from_documents(docs[:5], embedding=embedding_model, index_name=index_name)\n",
    "index_pc.add_documents(docs[5:])"
   ],
   "id": "49eac1fee6fcb257",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26c9725d-ef43-4e64-8600-d4d6cf5a864a',\n",
       " '5ab2dab8-fbd8-44d7-af89-6db7243804cc']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Retrieval\n",
    "Now, we will use the retrieval from **langchain's proxy for Pinecone** index."
   ],
   "id": "272de23cb7e756d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:47:01.253913Z",
     "start_time": "2024-06-22T11:47:01.235913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_similar_docs(query, k=2):\n",
    "    similar_docs = index_pc.similarity_search(query, k=2)\n",
    "    return similar_docs"
   ],
   "id": "6d5ad672f9e4691b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## EXTRA: Instantiating LLM that will use the Pinecone DB",
   "id": "754edf8b23117252"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:47:02.485726Z",
     "start_time": "2024-06-22T11:47:02.352726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's instantiate the LLM that will use the results of the similarity search on out PineconeDB in order to create prompts\n",
    "llm = HuggingFaceEndpoint(repo_id=\"bigscience/bloom\", temperature=1e-10)\n",
    "llm"
   ],
   "id": "4250ee00a781ffd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\User\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='bigscience/bloom', temperature=1e-10, model='bigscience/bloom', client=<InferenceClient(model='bigscience/bloom', timeout=120)>, async_client=<InferenceClient(model='bigscience/bloom', timeout=120)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:47:31.269185Z",
     "start_time": "2024-06-22T11:47:31.255151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")  # We will learn more about chains in future lecture\n",
    "\n",
    "# This function will help us get the answer to the question that we raise\n",
    "def get_answer(query, verbose=False):\n",
    "    relevant_docs = get_similar_docs(query)\n",
    "    if verbose:\n",
    "        print(\"RELEVANT DOCS: \", relevant_docs)\n",
    "    response = chain.run(input_documents=relevant_docs, question=query)\n",
    "    return response"
   ],
   "id": "94cb2ceda47fc6b0",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Questions",
   "id": "87396391ec94a7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T11:51:31.764285Z",
     "start_time": "2024-06-22T11:51:29.217542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "our_query = \"How is India's economy? Answer VERY shortly.\"\n",
    "answer = get_answer(our_query)\n",
    "answer"
   ],
   "id": "f0048140c44f6cbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" India's economy is growing rapidly. It is a service-oriented and industrialized economy. Major cities like Mumbai, Delhi, Bangalore, and Chennai are hubs of business and commerce, attracting investments and fostering innovation.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Structure the Output",
   "id": "8255b1d680b7a703"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:19:56.206225Z",
     "start_time": "2024-06-22T12:19:56.076181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "import re\n",
    "import json"
   ],
   "id": "d7c6db81bca655f5",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:22:01.042176Z",
     "start_time": "2024-06-22T13:22:01.019139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This helps us create a schema for the desired output format (prompt engineering)\n",
    "\n",
    "# Response schema defines the JSON of the output\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"question\", description=\"Question generated from provided input text data.\"),\n",
    "    ResponseSchema(name=\"choices\", description=\"Available options for a multiple-choice question in comma separated.\"),\n",
    "    ResponseSchema(name=\"answer\", description=\"Correct answer for the asked question.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "print(\"OUTPUT PARSER: \", output_parser)  # This is an object (we won't use this directly), but instead its format instruction\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"FORMAT INSTRUCTIONS: \", format_instructions)  # This will be added to the prompt (to tell the LLM how to format output)"
   ],
   "id": "511dc9abaca3bc51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT PARSER:  response_schemas=[ResponseSchema(name='question', description='Question generated from provided input text data.', type='string'), ResponseSchema(name='choices', description='Available options for a multiple-choice question in comma separated.', type='string'), ResponseSchema(name='answer', description='Correct answer for the asked question.', type='string')]\n",
      "FORMAT INSTRUCTIONS:  The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"question\": string  // Question generated from provided input text data.\n",
      "\t\"choices\": string  // Available options for a multiple-choice question in comma separated.\n",
      "\t\"answer\": string  // Correct answer for the asked question.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:22:17.050844Z",
     "start_time": "2024-06-22T13:22:17.024810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's create a prompt template\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "            When a text input is given by the user, please generate multiple choice questions from it along with the correct answer.\n",
    "            \\n{format_instructions}\\n{user_prompt}\n",
    "        \"\"\")\n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}  # This means that this will be determined right away, but also can be changed later (just pass 'format_instructions=format_instructions' to the user prompt)\n",
    ") # unlike PromptTemplate, ChatPromptTemplate can contain the conversation history such as SystemMessage, HumanMessage and AssistantMessage\n",
    "\n",
    "# Fill in the prompt\n",
    "final_query = prompt.format_prompt(user_prompt=answer)\n",
    "final_query  # To only print the messages, use 'final_query.to_messages()'"
   ],
   "id": "914788699c8dd284",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n            When a text input is given by the user, please generate multiple choice questions from it along with the correct answer.\\n            \\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"question\": string  // Question generated from provided input text data.\\n\\t\"choices\": string  // Available options for a multiple-choice question in comma separated.\\n\\t\"answer\": string  // Correct answer for the asked question.\\n}\\n```\\n India\\'s economy is growing rapidly. It is a service-oriented and industrialized economy. Major cities like Mumbai, Delhi, Bangalore, and Chennai are hubs of business and commerce, attracting investments and fostering innovation.\\n        ')])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:38:29.165684Z",
     "start_time": "2024-06-22T12:38:27.032148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_model = ChatOpenAI()\n",
    "\n",
    "final_query_output = chat_model.invoke(final_query.to_messages()).content\n",
    "print(final_query_output)"
   ],
   "id": "e1120b6cf42c286b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"question\": \"Which country's economy is growing rapidly?\",\n",
      "\t\"choices\": \"A. China, B. India, C. USA, D. Brazil\",\n",
      "\t\"answer\": \"B. India\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:38:29.733416Z",
     "start_time": "2024-06-22T12:38:29.712416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Structure the output into an actual json\n",
    "markdown_text = final_query_output\n",
    "json_string = re.search(r'{(.*?)}', markdown_text, re.DOTALL).group(1)\n",
    "json_string"
   ],
   "id": "bfff7b4299f434ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\t\"question\": \"Which country\\'s economy is growing rapidly?\",\\n\\t\"choices\": \"A. China, B. India, C. USA, D. Brazil\",\\n\\t\"answer\": \"B. India\"\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdcee2505f8a80c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
